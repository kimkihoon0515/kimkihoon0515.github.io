---
title: MLOps
categories: MLOps
toc: true
toc_sticky: true
toc_label: SageMaker,BentoML
---

## SageMaker
![image](https://user-images.githubusercontent.com/63439911/198020774-94203ef7-ec61-4c7d-98e0-4fc7e62dd142.png)
EC2 인스턴스 위에 설치한 완전 관리형 서비스이다. EC2 인스턴스 위에 Jupyter Notebook을 설치한 것

### 장점

- 클라우드 환경이라 고성능 컴퓨터가 필요없음
- ML 패키지 설치가 필요없음
- 코드 작성부터 배포까지 한 번에 가능

## BentoML

### Serving

개발된 모델을 서빙 하는 것이다.

크게 `Batch,` `Online,` `Edge(Mobile)` 로 나눠져있다. Serving 시에 의존성 관리를 중요하게 생각하기 때문에 Docker나 Kubernetes를 기반으로 한다.

- Batch Serving : 특정 주기로 서빙하는 것
    - Batch로 한꺼번에 많은 양을 처리함
    - Airflow나 Cronjob을 특정 주기 시간단위로 예측한다.
    - 예측 결과를 DB에 저장하고 서버에서 활용한다.
- Online Serving : API 서빙, 실시간 요청에 따른 반응을 함
    - 동시에 여러 요청에 대한 확장대책이 필요하고 Batch 처리가 불가능
    - Kfserving, BentoML, Tensorflow Serving, Seldon Core 등의 라이브러리가 있다.

### BentoML
![image](https://user-images.githubusercontent.com/63439911/198021167-60d6e1db-df00-489f-a8c9-e7ec4175144e.png)
적은 코드로 production 서비스까지 가능하다. 모델 관리를 위한 웹 대시보드 또한 존재한다.

Python Script로 작성해서 Airflow로 접근이 가능하다.

- 장점
    
    API 서버를 자동으로 만들어준다.
    
    모델 저장소와 배포 관리를 위한 웹 대시보드, swagger 제공
    
- Adaptive Micro Batching
    
    모델 서빙시 개별 추론 요청을 배치단위로 처리하는 것은 성능에 큰 영향을 준다.
    
    BentoML은 HTTP 처리 데이터 처리과정까지 Micro batching을 지원한다.


